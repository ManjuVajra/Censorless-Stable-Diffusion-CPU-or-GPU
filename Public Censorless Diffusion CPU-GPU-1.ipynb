{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fY8vEA1OXcDj"},"outputs":[],"source":["!pip install diffusers==0.2.4\n","!pip install transformers scipy ftfy\n","!pip install \"ipywidgets>=7,<8\"\n","\n","from google.colab import output\n","output.enable_custom_widget_manager()\n","\n","from huggingface_hub import notebook_login\n","notebook_login()\n","\n","import torch\n","torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","from PIL import Image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2QvREIqGXePw"},"outputs":[],"source":["from transformers import CLIPTextModel, CLIPTokenizer\n","from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n","\n","# 1. Load the autoencoder model which will be used to decode the latents into image space. \n","vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", use_auth_token=True)\n","\n","# 2. Load the tokenizer and text encoder to tokenize and encode the text. \n","tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n","text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n","\n","# 3. The UNet model for generating the latents.\n","unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", use_auth_token=True)\n","\n","from diffusers import LMSDiscreteScheduler\n","\n","scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n","\n","vae = vae.to(torch_device)\n","text_encoder = text_encoder.to(torch_device)\n","unet = unet.to(torch_device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eIcUdNEUXlWq"},"outputs":[],"source":["prompt = [\"Sexy Succubus Demoness, HDR, 8K resolution, 64 Megapixels\"]\n","\n","height = 512                       # default height of Stable Diffusion\n","width = 768                         # Upped from 512 default width of Stable Diffusion, lower to reduce render time \n","\n","num_inference_steps = 15            # Number of denoising steps, lower than 15 is more noise than picture \n","\n","guidance_scale = 12               # Scale for classifier-free guidance\n","\n","#generator = torch.Generator(torch_device)\n","\n","generator = torch.manual_seed(69)   # Seed generator to create the inital latent noise\n","\n","batch_size = 1"]},{"cell_type":"markdown","source":["Run After to Decensor."],"metadata":{"id":"Mg72OQYzftXg"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"H7FGN8LDXu6g"},"outputs":[],"source":["text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n","\n","with torch.no_grad():\n","  text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n","\n","max_length = text_input.input_ids.shape[-1]\n","uncond_input = tokenizer(\n","    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",")\n","with torch.no_grad():\n","  uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n","\n","text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n","\n","latents = torch.randn(\n","  (batch_size, unet.in_channels, height // 8, width // 8),\n","  #generator=generator,\n",")\n","latents = latents.to(torch_device)\n","\n","latents.shape\n","\n","scheduler.set_timesteps(num_inference_steps)\n","\n","latents = latents * scheduler.sigmas[0]\n","\n","\n","from tqdm.auto import tqdm\n","\n","for i, t in tqdm(enumerate(scheduler.timesteps)):\n","    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n","    latent_model_input = torch.cat([latents] * 2)\n","    sigma = scheduler.sigmas[i]\n","    latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n","\n","    # predict the noise residual\n","    with torch.no_grad():\n","      noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n","\n","    # perform guidance\n","    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n","    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n","\n","    # compute the previous noisy sample x_t -> x_t-1\n","    latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"]\n","\n","# scale and decode the image latents with vae\n","latents = 1 / 0.18215 * latents\n","\n","with torch.no_grad():\n","  image = vae.decode(latents)\n","\n","image = (image / 2 + 0.5).clamp(0, 1)\n","image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n","images = (image * 255).round().astype(\"uint8\")\n","pil_images = [Image.fromarray(image) for image in images]\n","pil_images[0]"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[{"file_id":"1TZ307U1v217KrJbHVQ1baSCASZ9CMo7i","timestamp":1664655575620}],"authorship_tag":"ABX9TyOxUTxhWWGUh6wDmKZwmLL8"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}